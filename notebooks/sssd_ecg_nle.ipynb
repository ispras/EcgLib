{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSSD-ECG-nle methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../\n",
    "# Install ecglib package\n",
    "%pip install .\n",
    "# Install the cauchy CUDA kernel (it's need to fast calculate s4 kernel)\n",
    "%pip install src/ecglib/models/architectures/sssd/extensions/cauchy/.\n",
    "%cd src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSSD-ECG-nle training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Callable\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ecglib.data import load_ptb_xl\n",
    "from ecglib.data import EcgDataset\n",
    "from ecglib.models.config.model_configs import SSSDConfig\n",
    "from ecglib.models.model_builder import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare PTB-XL ECG dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to change a logic of downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_unzip = '/home/ecg_data/physionet_data/raw_data/'\n",
    "SAMPLE_FREQUENCY = 100\n",
    "ptb_xl_info = load_ptb_xl(path_to_unzip=path_to_unzip, frequency=SAMPLE_FREQUENCY)\n",
    "ptb_xl_info['frequency'] = SAMPLE_FREQUENCY\n",
    "\n",
    "# Split in accordance with PTB-XL Benchmarking\n",
    "val_ptbxl_info = ptb_xl_info.loc[ptb_xl_info['strat_fold'] == 9].reset_index()\n",
    "test_ptbxl_info = ptb_xl_info.loc[ptb_xl_info['strat_fold'] == 10].reset_index()\n",
    "train_ptbxl_info = ptb_xl_info.loc[~ptb_xl_info['strat_fold'].isin([9, 10])].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure parameters of dataset\n",
    "@dataclass(repr=True)\n",
    "class DataParams:\n",
    "    syndromes: List[str] = field(default_factory=lambda: ['AFIB']) # CRBBB, 1AVB, PVC or any code from `scp_statements.csv`. For multilabel-scenario append to list\n",
    "    normalization: str = 'identical'\n",
    "    leads: List[int] = field(default_factory=lambda: [0, 5, 6, 7, 8, 9, 10, 11]) # Standart sequence ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']. \n",
    "                                                                                 # We take only 8 LI leads ['I', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    sample_frequency: int = SAMPLE_FREQUENCY # 500\n",
    "    augmentation: Callable = None # Check README.md Preprocessing for more details\n",
    "    batch_size: int = 8 # Train with batch_size=8 require 15Gb GPU memory\n",
    "\n",
    "data_config = DataParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the sequence of leads\n",
    "wfdb.rdsamp(ptb_xl_info.iloc[0]['fpath'])[1]['sig_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EcgDataset's\n",
    "def dataset_from_map_file(map_file):\n",
    "    targets = [[1.0 if synd in eval(map_file.iloc[i]['scp_codes']).keys() else 0.0 for synd in data_config.syndromes] \n",
    "                for i in range(map_file.shape[0])]\n",
    "    return EcgDataset(ecg_data=map_file, \n",
    "                      target=targets,\n",
    "                      frequency=data_config.sample_frequency,\n",
    "                      leads=data_config.leads,\n",
    "                      norm_type=data_config.normalization,\n",
    "                      classes=len(data_config.syndromes),\n",
    "                      augmentation=data_config.augmentation\n",
    "                    )\n",
    "\n",
    "train_dataset = dataset_from_map_file(train_ptbxl_info)\n",
    "val_dataset = dataset_from_map_file(val_ptbxl_info)\n",
    "test_dataset = dataset_from_map_file(test_ptbxl_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataloaders\n",
    "def dataloader_from_dataset(dataset, train=True):\n",
    "    return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=data_config.batch_size,\n",
    "            shuffle=train,\n",
    "            drop_last=train,\n",
    "        )\n",
    "\n",
    "train_loader = dataloader_from_dataset(train_dataset)\n",
    "val_loader = dataloader_from_dataset(val_dataset, train=False)\n",
    "test_loader = dataloader_from_dataset(test_dataset, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SSSD-ECG-nle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sssd_ecg_nle_config = SSSDConfig(\n",
    "    in_channels=len(data_config.leads),\n",
    "    label_embed_classes=len(data_config.syndromes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sssd_model = create_model(\n",
    "    model_name='sssd_ecg',\n",
    "    config=sssd_ecg_nle_config,\n",
    "    pathology=data_config.syndromes,\n",
    "    leads_count=len(data_config.leads),\n",
    "    num_classes=len(data_config.syndromes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure parameters of dataset\n",
    "@dataclass(repr=True)\n",
    "class TrainParams:\n",
    "    num_viewed_samples: int = 10**6 # Number of viewed samples before model training will stop\n",
    "    batch_size: int = data_config.batch_size\n",
    "    # Diffusion Hyperparams\n",
    "    T: int = 200 # denoising num steps\n",
    "    beta_0: float = 0.0001 # first beta in markov chain\n",
    "    beta_T: float = 0.02   # last beta. Linear interpolation between beta_0 and beta_T\n",
    "    grad_norm: float = None # Maximum norm to gradient norm clipping\n",
    "    grad_val: float = None # Maximum norm to gradient value clipping\n",
    "    lr: float = 0.0002\n",
    "\n",
    "train_config = TrainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_diffusion_hyperparams(T, beta_0, beta_T):\n",
    "        \"\"\"\n",
    "        Compute diffusion process hyperparameters\n",
    "\n",
    "        Parameters:\n",
    "        T (int):                    number of diffusion steps\n",
    "        beta_0 and beta_T (float):  beta schedule start/end value, \n",
    "                                    where any beta_t in the middle is linearly interpolated\n",
    "        \n",
    "        Returns:\n",
    "        a dictionary of diffusion hyperparameters including:\n",
    "            T (int), Beta/Alpha/Alpha_bar/Sigma (torch.tensor on cpu, shape=(T, ))\n",
    "            These cpu tensors are changed to cuda tensors on each individual gpu\n",
    "        \"\"\"\n",
    "\n",
    "        Beta = torch.linspace(beta_0, beta_T, T)  # Linear schedule\n",
    "        Alpha = 1 - Beta\n",
    "        Alpha_bar = Alpha + 0\n",
    "        Beta_tilde = Beta + 0\n",
    "        for t in range(1, T):\n",
    "            Alpha_bar[t] *= Alpha_bar[t - 1]  # \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\n",
    "            Beta_tilde[t] *= (1 - Alpha_bar[t - 1]) / (\n",
    "                    1 - Alpha_bar[t])  # \\tilde{\\beta}_t = \\beta_t * (1-\\bar{\\alpha}_{t-1})\n",
    "            # / (1-\\bar{\\alpha}_t)\n",
    "        Sigma = torch.sqrt(Beta_tilde)  # \\sigma_t^2  = \\tilde{\\beta}_t\n",
    "\n",
    "        _dh = {}\n",
    "        _dh[\"T\"], _dh[\"Beta\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Sigma\"] = T, Beta, Alpha, Alpha_bar, Sigma\n",
    "        return _dh\n",
    "\n",
    "train_config.diffusion_hyperparams = calc_diffusion_hyperparams(train_config.T, train_config.beta_0, train_config.beta_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader, train_config, device):\n",
    "    num_epochs = train_config.num_viewed_samples // (len(dataloader) * train_config.batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch number:\", epoch)\n",
    "        model.train()\n",
    "        num_curr_samples = 0 # counter for the number of examples viewed\n",
    "        for _, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            # Get batch\n",
    "            ids, (input, targets) = batch\n",
    "            ecg_signal = input[0].to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            # Get model input\n",
    "            T, Alpha_bar = train_config.diffusion_hyperparams[\"T\"], train_config.diffusion_hyperparams[\"Alpha_bar\"]\n",
    "            Alpha_bar = Alpha_bar.to(device)\n",
    "            diffusion_steps = torch.randint(T, size=(ecg_signal.shape[0],1)).to(device) # randomly sample diffusion steps from 1~T\n",
    "            z = torch.normal(0, 1, size=ecg_signal.shape).to(device)\n",
    "            transformed_X = torch.sqrt(Alpha_bar[diffusion_steps]) * ecg_signal + torch.sqrt(1-Alpha_bar[diffusion_steps]) * z\n",
    "            optimizer.zero_grad()\n",
    "            # Loss propagation\n",
    "            epsilon_theta = model((transformed_X, targets, None, diffusion_steps,))\n",
    "            loss = criterion(epsilon_theta, z)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if train_config.grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=train_config.grad_norm)\n",
    "\n",
    "            # Gradient Val Clipping\n",
    "            if train_config.grad_val is not None:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=train_config.grad_val)\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            # Interrupt process\n",
    "            num_curr_samples += ecg_signal.shape[0]\n",
    "            if num_curr_samples > train_config.num_viewed_samples:\n",
    "                return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize params\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.MSELoss()\n",
    "sssd_model = sssd_model.to(device)\n",
    "optimizer = torch.optim.Adam(params=sssd_model.parameters(), lr=train_config.lr)\n",
    "trained_sssd_model = train(sssd_model, optimizer, criterion, train_loader, train_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSSD-ECG-nle Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure parameters of dataset\n",
    "@dataclass(repr=True)\n",
    "class GenerationParams:\n",
    "    generation_path: str = 'afib_synthetic_ptbxl'\n",
    "    batch_size: int = 10\n",
    "    leads: List[int] = field(default_factory=lambda: data_config.leads)\n",
    "    ecg_length: int = 1000\n",
    "    T: int = train_config.T # denoising num steps\n",
    "    beta_0: float = train_config.beta_0 # first beta in markov chain\n",
    "    beta_T: float = train_config.beta_T   # last beta. Linear interpolation between beta_0 and beta_T\n",
    "    syndromes: List[str] = field(default_factory=lambda: data_config.syndromes) # CRBBB, 1AVB, PVC or any code from `scp_statements.csv`. For multilabel-scenario append to list\n",
    "    pass\n",
    "\n",
    "gen_config = GenerationParams()\n",
    "gen_config.diffusion_hyperparams = calc_diffusion_hyperparams(gen_config.T, gen_config.beta_0, gen_config.beta_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_label(net, size, diffusion_hyperparams, device, cond=None, metadata=None):\n",
    "    \"\"\"\n",
    "    Perform the complete sampling step according to p(x_0|x_T) = \\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_t)\n",
    "\n",
    "    Parameters:\n",
    "    net (torch network):            the wavenet model\n",
    "    size (tuple):                   size of tensor to be generated, \n",
    "                                    usually is (number of audios to generate, channels=1, length of audio)\n",
    "    diffusion_hyperparams (dict):   dictionary of diffusion hyperparameters returned by calc_diffusion_hyperparams\n",
    "                                    note, the tensors need to be cuda tensors \n",
    "    cond: conditioning as integer tensor\n",
    "    guidance_weight: weight for classifier-free guidance (if trained with conditioning_dropout>0)\n",
    "    \n",
    "    Returns:\n",
    "    the generated audio(s) in torch.tensor, shape=size\n",
    "    \"\"\"\n",
    "\n",
    "    _dh = diffusion_hyperparams\n",
    "    T, Alpha, Alpha_bar, Sigma = _dh[\"T\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Sigma\"]\n",
    "    bs = size[0]\n",
    "    assert len(Alpha) == T\n",
    "    assert len(Alpha_bar) == T\n",
    "    assert len(Sigma) == T\n",
    "    assert len(size) == 3\n",
    "\n",
    "    x = torch.normal(0, 1, size=size).to(device)\n",
    "    with torch.no_grad():\n",
    "        for t in range(T-1, -1, -1):\n",
    "            diffusion_steps = (t * torch.ones((bs, 1))).to(device)  # use the corresponding reverse step\n",
    "            epsilon_theta = net((x, cond, metadata, diffusion_steps,))  # predict \\epsilon according to \\epsilon_\\theta               \n",
    "            x = (x - (1-Alpha[t])/torch.sqrt(1-Alpha_bar[t]) * epsilon_theta) / torch.sqrt(Alpha[t])  # update x_{t-1} to \\mu_\\theta(x_t)\n",
    "            if t > 0:\n",
    "                x = x + Sigma[t] * torch.normal(0, 1, size=size).to(device)  # add the variance term to x_{t-1}\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_four_leads(tensor):\n",
    "    leadI = tensor[:,0,:].unsqueeze(1) # I = LA - RA\n",
    "    leadavf = tensor[:,1,:].unsqueeze(1) # 3/2(LL - V_w),    V_w = 1/3(RA + LA + LL)\n",
    "    leadschest = tensor[:,2:8,:] # Vi - V_w\n",
    "\n",
    "    leadII = (0.5*leadI) + leadavf # II = LL - RA\n",
    "\n",
    "    leadIII = -(0.5*leadI) + leadavf # III = LL - LA\n",
    "    leadavr = -(0.75*leadI) -(0.5*leadavf) # 3/2 (RA - V_w)\n",
    "    leadavl = (0.75*leadI) - (0.5*leadavf) # 3/2 (LA - V_w)\n",
    "\n",
    "    leads12 = torch.cat([leadI, leadII, leadIII, leadavr, leadavl, leadavf, leadschest], dim=1)\n",
    "\n",
    "    return leads12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(model, ecg_info, gen_config, device, mode='train'):\n",
    "    # Main pipeline\n",
    "    bar = tqdm(total=len(ecg_info))\n",
    "    bs = gen_config.batch_size\n",
    "    leads_num = len(gen_config.leads)\n",
    "    raw_df = ecg_info.copy()\n",
    "    if SAMPLE_FREQUENCY == 100:\n",
    "        postfix = 'lr' # low-rate\n",
    "    else:\n",
    "        postfix = 'hr' # high-rate\n",
    "    for i in range(0, len(ecg_info), bs):\n",
    "        # Get batch\n",
    "        if i + bs > len(ecg_info):\n",
    "            batch_df = ecg_info.iloc[i:]\n",
    "        else:\n",
    "            batch_df = ecg_info.iloc[i:i+bs]\n",
    "        # Get condition\n",
    "        targets = [[1.0 if synd in eval(ecg_info.iloc[i+j]['scp_codes']).keys() else 0.0 for synd in gen_config.syndromes] \n",
    "            for j in range(len(batch_df))]\n",
    "        cond = torch.tensor(targets).long().to(device)\n",
    "        # Sampling\n",
    "        generated_ecg = sampling_label(\n",
    "            model, \n",
    "            (len(batch_df), leads_num, gen_config.ecg_length),\n",
    "            gen_config.diffusion_hyperparams,\n",
    "            device,\n",
    "            cond,\n",
    "            metadata=None\n",
    "        )\n",
    "        if leads_num == 8:\n",
    "            generated_ecg12 = generate_four_leads(generated_ecg)\n",
    "        else:\n",
    "            assert leads_num == 12\n",
    "            generated_ecg12 = generated_ecg\n",
    "        # Saving\n",
    "        for j, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            file_name = '_'.join(row[f'filename_{postfix}'].split('/')[1:]) + '.npz'\n",
    "            fpath = os.path.join(gen_config.generation_path, 'data', file_name)\n",
    "            np.savez(fpath, generated_ecg12[j].detach().cpu().numpy())\n",
    "            raw_df.loc[i+j, 'fpath'] = fpath\n",
    "        # update bar\n",
    "        bar.update(len(batch_df))\n",
    "    raw_df.to_csv(os.path.join(gen_config.generation_path, f\"{gen_config.generation_path}_{mode}_map_file.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(gen_config.generation_path, 'data'), exist_ok=True)\n",
    "sssd_model.to(device)\n",
    "sssd_model.eval()\n",
    "generation(sssd_model, train_ptbxl_info, gen_config, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
